{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a. Machine learning with satellite features - all countries except India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression, RidgeClassifierCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import sem, ttest_ind, spearmanr\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "warnings.filterwarnings('ignore')\n",
    "import ray\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(args):\n",
    "    \n",
    "    # Nice process\n",
    "    p = psutil.Process(os.getpid())\n",
    "    p.nice(19)\n",
    "    \n",
    "    # Get arguments\n",
    "    i, train, test, TARGET, regression = args\n",
    "    print('Simulation %i' % i, end='\\r')\n",
    "    np.random.seed(i)\n",
    "    ALPHAS = np.logspace(-3, 3, 10)\n",
    "    \n",
    "    x_train, x_test = train[feature_cols], test[feature_cols]\n",
    "    y_train, y_test = train[TARGET], test[TARGET]\n",
    "    \n",
    "    # Fit and predict with Ridge regression\n",
    "    if regression == 'regression':\n",
    "        model = RidgeCV(alphas=ALPHAS)\n",
    "    else:\n",
    "        model = LogisticRegressionCV(Cs=6)\n",
    "    model.fit(x_train, y_train)\n",
    "    if regression == 'regression':\n",
    "        if model.alpha_ == ALPHAS[0]:\n",
    "            print('Warning: Selected alpha is at lower extreme')\n",
    "        if model.alpha_ == ALPHAS[-1]:\n",
    "            print('Warning: Selected alpha is at upper extreme')\n",
    "        \n",
    "        yhat_train = model.predict(x_train)\n",
    "        yhat_test = model.predict(x_test)\n",
    "    else:\n",
    "        yhat_train = model.predict_proba(x_train)[:, 0]\n",
    "        yhat_test = model.predict_proba(x_test)[:, 0]\n",
    "    \n",
    "    data_train = train.copy()\n",
    "    data_train['y'] = y_train\n",
    "    data_train['yhat'] = yhat_train\n",
    "    mse = np.mean(((np.array(yhat_test) - np.array(y_test))**2))\n",
    "    data_train['y_noised'] = np.array(y_train) + np.random.normal(0, np.sqrt(mse), len(y_train))\n",
    "    data_train['sim'] = i\n",
    "    data_train['split'] = 'train'\n",
    "    \n",
    "    data_test = test.copy()\n",
    "    data_test['y'] = y_test\n",
    "    data_test['yhat'] = yhat_test\n",
    "    mse = np.mean(((np.array(yhat_test) - np.array(y_test))**2))\n",
    "    data_test['y_noised'] = np.array(y_test) + np.random.normal(0, np.sqrt(mse), len(y_test))\n",
    "    data_test['sim'] = i\n",
    "    data_test['split'] = 'test'    \n",
    "    \n",
    "    data = pd.concat([data_train, data_test])\n",
    "    \n",
    "    return data[['y', 'yhat', 'y_noised', 'sim', 'split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in ['us', 'mexico', 'colombia', 'honduras', 'indonesia', 'kenya', 'nigeria', 'peru', 'philiippines']:\n",
    "    \n",
    "    print('Running country ' + country + '...')\n",
    "    print('=========================')\n",
    "\n",
    "\n",
    "    if country == 'us':\n",
    "        FEATURES_FNAME = '/data/mosaiks/replication/features/mosaiks_features_by_puma_us.csv'\n",
    "        LABELS_FNAME = '/data/mosaiks/replication/surveys/us/groundtruth_by_puma_2019.csv'\n",
    "        MERGE_KEY = 'StatePUMA'\n",
    "        SPLIT_KEYS = {'no_spatial':'StatePUMA'}\n",
    "        POVERTY = 'FINCP' \n",
    "        WEIGHT = 'PWGTP'\n",
    "        OUTFILE_NAME = '/data/mosaiks/replication/simulations/us/'\n",
    "    \n",
    "    elif country == 'mexico':\n",
    "        FEATURES_FNAME = '/data/mosaiks/replication/features/mosaiks_features_by_municipality_mexico.csv'\n",
    "        LABELS_FNAME = '/data/mosaiks/replication/surveys/mexico/grouped.csv'\n",
    "        MERGE_KEY = 'municipality'\n",
    "        SPLIT_KEYS = {'no_spatial':'municipality'}\n",
    "        POVERTY = 'asset_index' \n",
    "        WEIGHT = 'weight'\n",
    "        OUTFILE_NAME = '/data/mosaiks/replication/simulations/mexico/'\n",
    "    \n",
    "    else:\n",
    "        FEATURES_FNAME = '/data/mosaiks/replication/features/dhs/mosaiks_features_by_cluster_' + country + '.csv'\n",
    "        LABELS_FNAME = '/data/mosaiks/replication/surveys/dhs/' + country + '_grouped.csv'\n",
    "        MERGE_KEY = 'cluster'\n",
    "        SPLIT_KEYS = {'spatial':'region', 'no_spatial':'cluster'}\n",
    "        POVERTY = 'wealth' \n",
    "        WEIGHT = 'weight'\n",
    "        OUTFILE_NAME = '/data/mosaiks/replication/simulations/dhs/' + country + '/'\n",
    "        \n",
    "    for method, TARGET in [('regression', POVERTY), ('classification', 'rural')]:\n",
    "        for spatial in SPLIT_KEYS.keys():\n",
    "            print('Running ' + method + ' with ' + spatial + '...')\n",
    "            print('--------------------')\n",
    "\n",
    "            # Load and merge data\n",
    "            features = pd.read_csv(FEATURES_FNAME)\n",
    "            print('Regions with features: %i' % len(features))\n",
    "            labels = pd.read_csv(LABELS_FNAME)\n",
    "            \n",
    "            # Normalize labels\n",
    "            if country == 'us' and method == 'regression':\n",
    "                labels[TARGET] = np.log(labels[TARGET] + 1)\n",
    "                \n",
    "            if method == 'regression':\n",
    "                label_mean, label_std = labels[TARGET].mean(), labels[TARGET].std()\n",
    "                labels[TARGET] = (labels[TARGET] - label_mean)/label_std\n",
    "            \n",
    "            if country == 'us':\n",
    "                features['StatePUMA'] = features['State'] + features['PUMA'].apply(lambda x: str(int(x)))\n",
    "                labels['StatePUMA'] = labels['State'] + labels['PUMA'].apply(lambda x: str(int(x)))\n",
    "                \n",
    "            print('Regions with labels: %i' % len(labels))\n",
    "            df = labels.merge(features, on=MERGE_KEY, how='inner')\n",
    "            print('Regions with features and labels: %i' % len(df))\n",
    "            feature_cols = [c for c in df.columns if 'Feature' in c]\n",
    "            \n",
    "            # Preprocess MOSAIKS features: drop columns with all nulls or all same value, standardize\n",
    "            for col in feature_cols:\n",
    "                cols_to_drop = []\n",
    "                if df[col].isnull().sum() == len(df) or df[col].std() == 0:\n",
    "                    cols_to_drop.append(col)\n",
    "                else:\n",
    "                    df[col] = (df[col] - df[col].mean())/df[col].std()\n",
    "            df = df.drop(cols_to_drop, axis=1)\n",
    "            \n",
    "            SPLIT_KEY = SPLIT_KEYS[spatial]\n",
    "            trains, tests = [], []\n",
    "        \n",
    "            for i in range(100):\n",
    "                if SPLIT_KEY == 'buffer':\n",
    "                    train_states = list(pd.read_csv('/data/mosaiks/splits/' + country + '/train_' + str(i+1) \n",
    "                                                    + '.csv')['region'])\n",
    "                    test_states = list(pd.read_csv('/data/mosaiks/splits/' + country + '/test_' + str(i+1) \n",
    "                                                   + '.csv')['region'])\n",
    "                    train = df[df[MERGE_KEY].isin(train_states)]\n",
    "                    test = df[df[MERGE_KEY].isin(test_states)]\n",
    "\n",
    "                else:\n",
    "                    states = list(df[SPLIT_KEY].unique())\n",
    "                    train_states, test_states = train_test_split(states, test_size=0.25, random_state=i)\n",
    "                    train = df[df[SPLIT_KEY].isin(train_states)]\n",
    "                    test = df[df[SPLIT_KEY].isin(test_states)]\n",
    "            \n",
    "                trains.append(train)\n",
    "                tests.append(test)\n",
    "                \n",
    "            if method == 'regression':\n",
    "                df[[MERGE_KEY]].to_csv(OUTFILE_NAME + 'ids.csv', index=True)\n",
    "            \n",
    "            results = [simulation((i, trains[i], tests[i], TARGET, method)) for i in range(100)]\n",
    "            results = pd.concat(results)\n",
    "            results.to_csv(OUTFILE_NAME + TARGET + '_' + spatial + '.csv')\n",
    "            clear_output(wait=True)\n",
    "            print('Done!', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
